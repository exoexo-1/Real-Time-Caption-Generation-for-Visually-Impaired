{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c080ff9-27de-4080-bb7e-419813c44039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: opencv-python 4.11.0.86\n",
      "Uninstalling opencv-python-4.11.0.86:\n",
      "  Successfully uninstalled opencv-python-4.11.0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping opencv-python-headless as it is not installed.\n",
      "WARNING: Skipping opencv-contrib-python as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Using cached opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl (39.5 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install dependencies and imports\n",
    "# This cell handles package installation and imports all necessary libraries\n",
    "\n",
    "# First uninstall any existing OpenCV installations\n",
    "!pip uninstall opencv-python opencv-python-headless opencv-contrib-python -y\n",
    "\n",
    "# Install the full version with GUI support\n",
    "!pip install opencv-python\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import threading\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForQuestionAnswering\n",
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "import queue\n",
    "import easyocr\n",
    "import os\n",
    "\n",
    "# Force OpenCV to use a specific backend\n",
    "os.environ[\"OPENCV_VIDEOIO_PRIORITY_MSMF\"] = \"0\"  # For Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be4100d-8002-4536-a7b9-6f29d3136a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Class definition - Initialization and setup\n",
    "# This contains the class definition with initialization code\n",
    "\n",
    "class RealtimeVisionAssistant:\n",
    "    def __init__(self):\n",
    "        print(\"Initializing models. This may take a moment...\")\n",
    "        try:\n",
    "            # Vision models\n",
    "            self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "            self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "            \n",
    "            # VQA model\n",
    "            self.vqa_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-capfilt-large\")\n",
    "            self.vqa_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-capfilt-large\")\n",
    "            \n",
    "            # OCR\n",
    "            self.reader = easyocr.Reader(['en'])\n",
    "            \n",
    "            # Speech recognition and synthesis\n",
    "            self.recognizer = sr.Recognizer()\n",
    "            self.engine = None  # will init in thread\n",
    "            \n",
    "            # Common objects\n",
    "            self.object_classes = [\n",
    "                \"person\", \"bicycle\", \"car\", \"motorcycle\", \"bus\", \"truck\", \"traffic light\",\n",
    "                \"fire hydrant\", \"stop sign\", \"bench\", \"chair\", \"couch\", \"table\", \"door\",\n",
    "                \"stairs\", \"elevator\", \"bottle\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\",\n",
    "                \"laptop\", \"phone\", \"keyboard\", \"microwave\", \"oven\", \"sink\", \"refrigerator\", \"book\"\n",
    "            ]\n",
    "            \n",
    "            self.important_objects = [\"person\", \"car\", \"bicycle\", \"dog\", \"stairs\", \"door\", \"obstacle\"]\n",
    "            \n",
    "            print(\"Models loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing: {e}\")\n",
    "            raise\n",
    "\n",
    "        # States\n",
    "        self.current_frame = None\n",
    "        self.last_processed_frame = None\n",
    "        self.last_analysis_time = 0\n",
    "        self.conversation_history = []\n",
    "        self.running = False\n",
    "        self.camera = None\n",
    "\n",
    "        # Queues\n",
    "        self.speech_queue = queue.Queue()\n",
    "        self.command_queue = queue.Queue()\n",
    "        \n",
    "        # Settings\n",
    "        self.analysis_interval = 3.0\n",
    "        self.auto_describe = False\n",
    "        self.last_description_time = 0\n",
    "        self.voice_mode = \"casual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e19f3aef-f844-4556-abcd-1436d404732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Camera control methods\n",
    "# This cell contains methods for starting/stopping the camera\n",
    "\n",
    "def start_camera(self):\n",
    "    try:\n",
    "        self.camera = cv2.VideoCapture(0)\n",
    "        time.sleep(2)\n",
    "        if not self.camera.isOpened():\n",
    "            self.speak(\"Camera failed to initialize. Please check connections.\")\n",
    "            return False\n",
    "        self.running = True\n",
    "        print(\"Camera started successfully\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Camera error: {e}\")\n",
    "        self.speak(\"Camera failed to start\")\n",
    "        return False\n",
    "\n",
    "def stop_camera(self):\n",
    "    self.running = False\n",
    "    if self.camera:\n",
    "        self.camera.release()\n",
    "    try:\n",
    "        cv2.destroyAllWindows()\n",
    "    except:\n",
    "        pass  # Ignore if windows can't be destroyed\n",
    "    print(\"Camera stopped\")\n",
    "\n",
    "# Add these methods to the class\n",
    "RealtimeVisionAssistant.start_camera = start_camera\n",
    "RealtimeVisionAssistant.stop_camera = stop_camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a027de85-4bb4-4590-b986-0f083aba4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Speech and voice methods\n",
    "# This cell handles speech synthesis and voice recognition\n",
    "\n",
    "def speak(self, text):\n",
    "    print(f\"Assistant: {text}\")\n",
    "    self.speech_queue.put(text)\n",
    "\n",
    "def speech_worker(self):\n",
    "    try:\n",
    "        self.engine = pyttsx3.init()\n",
    "        self.engine.setProperty('rate', 180)\n",
    "        while self.running:\n",
    "            try:\n",
    "                if not self.speech_queue.empty():\n",
    "                    text = self.speech_queue.get()\n",
    "                    self.engine.say(text)\n",
    "                    self.engine.runAndWait()\n",
    "                else:\n",
    "                    time.sleep(0.1)\n",
    "            except Exception as e:\n",
    "                print(f\"Speech error: {e}\")\n",
    "                time.sleep(0.5)\n",
    "    except Exception as e:\n",
    "        print(f\"Speech worker error: {e}\")\n",
    "\n",
    "def listen_for_commands(self):\n",
    "    print(\"Voice recognition active. Listening for commands...\")\n",
    "    with sr.Microphone() as source:\n",
    "        self.recognizer.adjust_for_ambient_noise(source, duration=1)\n",
    "        while self.running:\n",
    "            try:\n",
    "                print(\"Listening...\")\n",
    "                audio = self.recognizer.listen(source, timeout=5, phrase_time_limit=5)\n",
    "                print(\"Processing speech...\")\n",
    "                try:\n",
    "                    command = self.recognizer.recognize_google(audio).lower()\n",
    "                    print(f\"You said: {command}\")\n",
    "                    self.command_queue.put(command)\n",
    "                except sr.UnknownValueError:\n",
    "                    pass\n",
    "                except sr.RequestError as e:\n",
    "                    print(f\"Speech recognition service error: {e}\")\n",
    "            except sr.WaitTimeoutError:\n",
    "                pass\n",
    "            except Exception as e:\n",
    "                print(f\"Listening error: {e}\")\n",
    "                time.sleep(0.5)\n",
    "\n",
    "# Add these methods to the class\n",
    "RealtimeVisionAssistant.speak = speak\n",
    "RealtimeVisionAssistant.speech_worker = speech_worker\n",
    "RealtimeVisionAssistant.listen_for_commands = listen_for_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81dff1e3-49c1-4052-b77c-71c149c4d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Vision processing methods\n",
    "# This cell contains methods for object detection, text recognition, and analysis\n",
    "\n",
    "def detect_text(self, image):\n",
    "    try:\n",
    "        if not isinstance(image, np.ndarray):\n",
    "            image = np.array(image)\n",
    "        results = self.reader.readtext(image)\n",
    "        return [text for _, text, conf in results if conf > 0.3]\n",
    "    except Exception as e:\n",
    "        print(f\"Text detection error: {e}\")\n",
    "        return []\n",
    "\n",
    "def identify_objects(self, image):\n",
    "    try:\n",
    "        inputs = self.clip_processor(\n",
    "            text=[\"a photo of a \" + obj for obj in self.object_classes],\n",
    "            images=image, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = self.clip_model(**inputs)\n",
    "            logits_per_image = outputs.logits_per_image\n",
    "            probs = logits_per_image.softmax(dim=1)\n",
    "        detected_objects = []\n",
    "        for i, obj in enumerate(self.object_classes):\n",
    "            confidence = probs[0][i].item()\n",
    "            if confidence > 0.3:\n",
    "                detected_objects.append((obj, confidence))\n",
    "        detected_objects.sort(key=lambda x: x[1], reverse=True)\n",
    "        return detected_objects\n",
    "    except Exception as e:\n",
    "        print(f\"Object detection error: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_description(self, image):\n",
    "    if image is None:\n",
    "        return \"No image available to analyze\"\n",
    "    text_elements = self.detect_text(image)\n",
    "    objects = self.identify_objects(image)\n",
    "    priority_objects = [obj for obj, conf in objects if obj in self.important_objects]\n",
    "    other_objects = [obj for obj, conf in objects if obj not in self.important_objects][:3]\n",
    "\n",
    "    description_parts = []\n",
    "    if any(obj == \"person\" for obj, _ in objects):\n",
    "        num_people = sum(1 for obj, _ in objects if obj == \"person\")\n",
    "        if num_people == 1:\n",
    "            description_parts.append(\"There's a person in front of you\")\n",
    "        else:\n",
    "            description_parts.append(f\"There are about {num_people} people in front of you\")\n",
    "    important = [obj for obj in priority_objects if obj != \"person\"]\n",
    "    if important:\n",
    "        if len(important) == 1:\n",
    "            description_parts.append(f\"I can see a {important[0]}\")\n",
    "        else:\n",
    "            obj_text = \", \".join(important[:-1]) + f\" and a {important[-1]}\"\n",
    "            description_parts.append(f\"I can see a {obj_text}\")\n",
    "    if self.voice_mode == \"detailed\" and other_objects:\n",
    "        if len(other_objects) == 1:\n",
    "            description_parts.append(f\"There's also a {other_objects[0]}\")\n",
    "        else:\n",
    "            obj_text = \", \".join(other_objects[:-1]) + f\" and a {other_objects[-1]}\"\n",
    "            description_parts.append(f\"There are also {obj_text}\")\n",
    "    if text_elements:\n",
    "        if len(text_elements) == 1 and len(text_elements[0]) < 50:\n",
    "            description_parts.append(f\"I can read the text: '{text_elements[0]}'\")\n",
    "        elif len(text_elements) > 0:\n",
    "            description_parts.append(f\"I can see some text that includes '{text_elements[0]}'\")\n",
    "\n",
    "    if not description_parts:\n",
    "        return \"I can see an image, but I can't clearly identify specific objects or text in it.\"\n",
    "    description = \". \".join(description_parts) + \".\"\n",
    "    return description\n",
    "\n",
    "# Add these methods to the class\n",
    "RealtimeVisionAssistant.detect_text = detect_text\n",
    "RealtimeVisionAssistant.identify_objects = identify_objects\n",
    "RealtimeVisionAssistant.generate_description = generate_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a204bf-b121-41aa-931e-35d35cb6512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Command processing and question answering\n",
    "# This cell handles user commands and visual question answering\n",
    "\n",
    "def process_command(self, command):\n",
    "    if any(x in command for x in [\"stop\", \"quit\", \"exit\"]):\n",
    "        self.speak(\"Shutting down assistant\")\n",
    "        self.running = False\n",
    "        return\n",
    "    \n",
    "    if \"describe\" in command or \"what do you see\" in command:\n",
    "        self.analyze_current_frame(force=True)\n",
    "        return\n",
    "    \n",
    "    if \"auto describe on\" in command or \"start describing\" in command:\n",
    "        self.auto_describe = True\n",
    "        self.speak(\"Automatic descriptions turned on\")\n",
    "        return\n",
    "    \n",
    "    if \"auto describe off\" in command or \"stop describing\" in command:\n",
    "        self.auto_describe = False\n",
    "        self.speak(\"Automatic descriptions turned off\")\n",
    "        return\n",
    "    \n",
    "    if \"casual mode\" in command:\n",
    "        self.voice_mode = \"casual\"\n",
    "        self.speak(\"Switched to casual description mode\")\n",
    "        return\n",
    "    \n",
    "    if \"detailed mode\" in command:\n",
    "        self.voice_mode = \"detailed\"\n",
    "        self.speak(\"Switched to detailed description mode\")\n",
    "        return\n",
    "    \n",
    "    self.answer_question(command)\n",
    "\n",
    "def answer_question(self, question):\n",
    "    if self.current_frame is None:\n",
    "        self.speak(\"I don't have an image to analyze at the moment.\")\n",
    "        return\n",
    "\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(self.current_frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    if any(keyword in question.lower() for keyword in [\"what do you see\", \"describe\", \"what's there\"]):\n",
    "        description = self.generate_description(pil_image)\n",
    "        self.speak(description)\n",
    "        return\n",
    "\n",
    "    if any(keyword in question.lower() for keyword in [\"text\", \"read\", \"say\", \"writing\"]):\n",
    "        text = self.detect_text(pil_image)\n",
    "        if text:\n",
    "            self.speak(f\"I can read the following text: {', '.join(text[:3])}\")\n",
    "        else:\n",
    "            self.speak(\"I don't see any readable text in view right now.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        inputs = self.vqa_processor(pil_image, question, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            output = self.vqa_model.generate(**inputs, max_length=50)\n",
    "        answer = self.vqa_processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        if len(answer) < 5:\n",
    "            self.speak(f\"It looks like {answer}\")\n",
    "        else:\n",
    "            self.speak(answer)\n",
    "    except Exception as e:\n",
    "        print(f\"VQA error: {e}\")\n",
    "        self.speak(\"I'm having trouble answering that.\")\n",
    "\n",
    "def analyze_current_frame(self, force=False):\n",
    "    current_time = time.time()\n",
    "    if not force and (current_time - self.last_description_time < self.analysis_interval):\n",
    "        return\n",
    "    self.last_description_time = current_time\n",
    "    if self.current_frame is not None:\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(self.current_frame, cv2.COLOR_BGR2RGB))\n",
    "        description = self.generate_description(pil_image)\n",
    "        self.speak(description)\n",
    "\n",
    "# Add these methods to the class\n",
    "RealtimeVisionAssistant.process_command = process_command\n",
    "RealtimeVisionAssistant.answer_question = answer_question\n",
    "RealtimeVisionAssistant.analyze_current_frame = analyze_current_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "170b8643-4c39-4e02-8a94-c57796f38935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Realtime Vision Assistant...\n",
      "Initializing models. This may take a moment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully\n",
      "Camera started successfully\n",
      "Voice recognition active. Listening for commands...\n",
      "Assistant: Vision assistant is active and ready to help.\n",
      "Assistant: Say 'describe' or 'what do you see' when you want me to describe the scene.\n",
      "Listening...\n",
      "Processing speech...\n",
      "Listening...\n",
      "Processing speech...\n",
      "Listening...\n",
      "Processing speech...\n",
      "You said: describe\n",
      "Listening...\n",
      "Listening...\n",
      "Assistant: There's a person in front of you.\n",
      "Processing speech...\n",
      "Listening...\n",
      "Processing speech...\n",
      "You said: describe\n",
      "Listening...\n",
      "Listening...\n",
      "Assistant: I can see some text that includes 'cello'.\n",
      "Processing speech...\n",
      "Listening...\n",
      "Processing speech...\n",
      "You said: detailed mode\n",
      "Listening...\n",
      "Assistant: Switched to detailed description mode\n",
      "Processing speech...\n",
      "Listening...\n",
      "Listening...\n",
      "Processing speech...\n",
      "You said: describe\n",
      "Listening...\n",
      "Listening...\n",
      "Assistant: There's also a phone. I can see some text that includes 'JUNE2025'.\n",
      "Processing speech...\n",
      "Listening...\n",
      "Processing speech...\n",
      "You said: where am i\n",
      "Listening...\n",
      "Assistant: living room\n",
      "Processing speech...\n",
      "Listening...\n",
      "Listening...\n",
      "Processing speech...\n",
      "You said: describe\n",
      "Listening...\n",
      "Listening...\n",
      "Assistant: There's also a cup. I can read the text: '2025'.\n",
      "Processing speech...\n",
      "Listening...\n",
      "Processing speech...\n",
      "Listening...\n",
      "Processing speech...\n",
      "You said: describe\n",
      "Listening...\n",
      "Listening...\n",
      "Assistant: There's also a book. I can see some text that includes 'Untamed'.\n",
      "Processing speech...\n",
      "Listening...\n",
      "Processing speech...\n",
      "Listening...\n",
      "Processing speech...\n",
      "You said: what colour is my t-shirt\n",
      "Listening...\n",
      "Assistant: yellow\n",
      "Listening...\n",
      "Processing speech...\n",
      "You said: describe\n",
      "Listening...\n",
      "Listening...\n",
      "Assistant: There's also a bottle. I can read the text: 'JuNE2025'.\n",
      "Processing speech...\n",
      "Listening...\n",
      "Processing speech...\n",
      "You said: exit\n",
      "Listening...\n",
      "Assistant: Shutting down assistant\n",
      "Camera stopped\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Main execution loop\n",
    "# This cell contains the main run method and execution code\n",
    "\n",
    "def run(self):\n",
    "    \"\"\"Main method to start all components\"\"\"\n",
    "    if not self.start_camera():\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        # Start speech and listening threads\n",
    "        threading.Thread(target=self.speech_worker, daemon=True).start()\n",
    "        threading.Thread(target=self.listen_for_commands, daemon=True).start()\n",
    "\n",
    "        # Welcome message\n",
    "        self.speak(\"Vision assistant is active and ready to help.\")\n",
    "        self.speak(\"Say 'describe' or 'what do you see' when you want me to describe the scene.\")\n",
    "\n",
    "        # Main processing loop\n",
    "        while self.running:\n",
    "            ret, frame = self.camera.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "                \n",
    "            self.current_frame = frame\n",
    "            \n",
    "            try:\n",
    "                # Display the frame\n",
    "                cv2.imshow('Vision Assistant', frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit\n",
    "                    self.running = False\n",
    "                    break\n",
    "            except:\n",
    "                # If display fails, continue without it\n",
    "                pass\n",
    "            \n",
    "            # Process any pending commands\n",
    "            if not self.command_queue.empty():\n",
    "                command = self.command_queue.get()\n",
    "                self.process_command(command)\n",
    "            \n",
    "            # Auto-describe if enabled\n",
    "            if self.auto_describe:\n",
    "                self.analyze_current_frame()\n",
    "                \n",
    "            time.sleep(0.05)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted by user\")\n",
    "    finally:\n",
    "        self.stop_camera()\n",
    "\n",
    "# Add the method to the class\n",
    "RealtimeVisionAssistant.run = run\n",
    "\n",
    "# Main execution\n",
    "print(\"Starting Realtime Vision Assistant...\")\n",
    "assistant = RealtimeVisionAssistant()\n",
    "assistant.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a3bac-8344-43b8-b19f-6d8dce8b0107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
